{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Homework \n",
    "***\n",
    "**Name**: $<$**Juan Lin**$>$ \n",
    "\n",
    "**Kaggle Username**: $<$**mzcolor**$>$\n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday February 23rd**. Additionally, you must make at least one submission to the **Kaggle** competition before it closes at **4:59pm on Friday February 23rd**. Submit only this Jupyter notebook to Moodle. Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "When people are discussing popular media, there’s a concept of spoilers. That is, critical information about the plot of a TV show, book, or movie that “ruins” the experience for people who haven’t read / seen it yet.\n",
    "\n",
    "The goal of this assignment is to do text classification on forum posts from the website [tvtropes.org](http://tvtropes.org/), to predict whether a post is a spoiler or not. We'll be using the logistic regression classifier provided by sklearn.\n",
    "\n",
    "Unlike previous assignments, the code provided with this assignment has all of the functionality required. Your job is to make the functionality better by improving the features the code uses for text classification.\n",
    "\n",
    "**NOTE**: Because the goal of this assignment is feature engineering, not classification algorithms, you may not change the underlying algorithm or it's parameters\n",
    "\n",
    "This assignment is structured in a way that approximates how classification works in the real world: Features are typically underspecified (or not specified at all). You, the data digger, have to articulate the features you need. You then compete against others to provide useful predictions.\n",
    "\n",
    "It may seem straightforward, but do not start this at the last minute. There are often many things that go wrong in testing out features, and you'll want to make sure your features work well once you've found them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle In-Class Competition \n",
    "***\n",
    "\n",
    "In addition to turning in this notebook on Moodle, you'll also need to submit your predictions on Kaggle, an online tournament site for machine learning competitions. The competition page can be found here:  \n",
    "\n",
    "[https://www.kaggle.com/c/feature-engineering-csci-5622-spring-2018](https://www.kaggle.com/c/feature-engineering-csci-5622-spring-2018)\n",
    "\n",
    "Additionally, a private invite link for the competition has been posted to Piazza. \n",
    "\n",
    "The starter code below has a `model_predict` method which produces a two column CSV file that is correctly formatted for Kaggle (predictions.csv). It should have the example Id as the first column and the prediction (`True` or `False`) as the second column. If you change this format your submissions will be scored as zero accuracy on Kaggle. \n",
    "\n",
    "**Note**: You may only submit **THREE** predictions to Kaggle per day.  Instead of using the public leaderboard as your sole evaluation processes, **it is highly recommended that you perform local evaluation using a validation set or cross-validation. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 1: Feature Engineering \n",
    "***\n",
    "\n",
    "The `FeatEngr` class is where the magic happens.  In it's current form it will read in the training data and vectorize it using simple Bag-of-Words.  It then trains a model and makes predictions.  \n",
    "\n",
    "25 points of your grade will be generated from your performance on the the classification competition on Kaggle. The performance will be evaluated on accuracy on the held-out test set. Half of the test set is used to evaluate accuracy on the public leaderboard.  The other half of the test set is used to evaluate accuracy on the private leaderboard (which you will not be able to see until the close of the competition). \n",
    "\n",
    "You should be able to significantly improve on the baseline system (i.e. the predictions made by the starter code we've provided) as reported by the Kaggle system.  Additionally, the top **THREE** students from the **PRIVATE** leaderboard at the end of the contest will receive 5 extra credit points towards their Problem 1 score.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Problem 1 Answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "import random\n",
    "import re\n",
    "import nltk\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import FunctionTransformer, Normalizer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatEngr:\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.vectorizer = FeatureUnion([       \n",
    "#                 Feature 1: sentence words using countvectorizer\n",
    "#                 ('sentence words tfidf', \n",
    "#                   Pipeline([('extract_sentence_column', FunctionTransformer(lambda x: x[0], validate = False)),\n",
    "#                             ('count_sentence', CountVectorizer(ngram_range=(1,2), token_pattern=r'\\b\\w+\\b', min_df=1)),\n",
    "#                 Feature 2: sentence words using tf-idf\n",
    "                ('sentence words tfidf', \n",
    "                  Pipeline([('extract_sentence_column', FunctionTransformer(lambda x: x[0], validate = False)),\n",
    "                            ('count_sentence', TfidfVectorizer(analyzer='word', ngram_range=(1,2), lowercase=True, norm='l2', stop_words='english'))])),            \n",
    "#                 Feature 3: trope column using countvercotrizer\n",
    "                ('trope type', \n",
    "                  Pipeline([('extract_trope_column', FunctionTransformer(lambda x: x[2], validate = False)),\n",
    "                            ('count_trope', TfidfVectorizer(analyzer='word', ngram_range=(1,2), lowercase=True, norm='l2', stop_words='english'))])), \n",
    "#                Feature 4: page words in sentence column, customized Transformer\n",
    "#                 ('page words in sentence column',             \n",
    "#                   Pipeline([('extract_page_sentence_columns', FunctionTransformer(lambda x: [x[0], x[1]], validate = False)), \n",
    "#                             ('count_page_sentence', PageWordsTransformer())])),\n",
    "            ])\n",
    "\n",
    "    def build_train_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in training text features and do further feature engineering \n",
    "        Most of the work in this homework will go here, or in similar functions  \n",
    "        :param examples: currently just a list of forum posts\n",
    "        vectorizer is to tokenize and count the word occurrences of a corpus of text documents.\n",
    "        \"\"\"\n",
    "        return self.vectorizer.fit_transform(examples)\n",
    "\n",
    "    def get_test_features(self, examples):\n",
    "        \"\"\"\n",
    "        Method to take in test text features and transform the same way as train features \n",
    "        :param examples: currently just a list of forum posts  \n",
    "        \"\"\"\n",
    "        return self.vectorizer.transform(examples)\n",
    "    \n",
    "    def show_top10(self):\n",
    "        \"\"\"\n",
    "        prints the top 10 features for the positive class and the \n",
    "        top 10 features for the negative class. \n",
    "        \"\"\"\n",
    "        feature_names = np.asarray(self.vectorizer.get_feature_names())\n",
    "        top10 = np.argsort(self.logreg.coef_[0])[-7:]\n",
    "        bottom10 = np.argsort(self.logreg.coef_[0])[:7]\n",
    "        print(\"Pos: %s\" % \" \".join(feature_names[top10]))\n",
    "        print(\"Neg: %s\" % \" \".join(feature_names[bottom10]))\n",
    "\n",
    "    \n",
    "    def train_predict_model(self, random_state=1234):\n",
    "        \"\"\"\n",
    "        Method to read in training data from file, and \n",
    "        train Logistic Regression classifier. \n",
    "        \n",
    "        :param random_state: seed for random number generator \n",
    "        \"\"\"\n",
    "        \n",
    "        from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "        # get training features and labels \n",
    "        dfTrain = pd.read_csv(\"../data/spoilers/train.csv\")  \n",
    "\n",
    "        temp = [list(dfTrain['sentence']), list(dfTrain['page']), list(dfTrain['trope'])]\n",
    "\n",
    "        self.X_train = self.build_train_features(temp)\n",
    "                                                 \n",
    "        self.y_train = np.array(dfTrain[\"spoiler\"], dtype=int)\n",
    "        \n",
    "        # train logistic regression model.  !!You MAY NOT CHANGE THIS!! \n",
    "        self.logreg = LogisticRegression(random_state=random_state)\n",
    "        self.logreg.fit(self.X_train, self.y_train)\n",
    "        \n",
    "        # get test features\n",
    "        dfTest = pd.read_csv(\"../data/spoilers/test.csv\")\n",
    "       \n",
    "        temp1 = (list(dfTest['sentence']), list(dfTest['page']), list(dfTest['trope']))\n",
    "\n",
    "        self.X_test = self.get_test_features(temp1)\n",
    "                 \n",
    "        pred = self.logreg.predict(self.X_test)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # dump predictions to file for submission to Kaggle  \n",
    "        aa = pd.DataFrame({\"spoiler\": np.array(pred, dtype=bool)}).to_csv(\"prediction.csv\", index=True, index_label=\"Id\")\n",
    "        \"\"\"\n",
    "        There are two approaches for validation, one is cross validation with fold = 5,\n",
    "        another one is score method using train_test_split with sklearn.\n",
    "        \"\"\"        \n",
    "#         calculate accuracy using cross validation\n",
    "        from sklearn.model_selection import cross_val_score\n",
    "        scores = cross_val_score(self.logreg, self.X_train, self.y_train, cv=5)\n",
    "        print(\"The score for each fold:\", scores)\n",
    "        print(\"The average accuracy is: %0.3f with %0.3f deviation\" % (scores.mean(), scores.std() * 2))\n",
    "        \n",
    "#         from sklearn.model_selection import train_test_split        \n",
    "#         X_train_crova, X_test_crova, y_train_crova, y_test_crova = train_test_split(self.X_train, self.y_train, test_size=0.2, random_state=None, shuffle=True)\n",
    "#         clf = self.logreg.fit(X_train_crova, y_train_crova)\n",
    "#         accuracy_score = clf.score(X_test_crova, y_test_crova)\n",
    "#         print(\"The accuracy score is: %0.3f\" % accuracy_score)\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find the words in page columns appear in the sentence column\n",
    "class PageWordsTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, example):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, example):\n",
    "        \n",
    "        import numpy as np\n",
    "        from scipy.sparse import csr_matrix\n",
    "       \n",
    "        X = np.zeros((len(example[0]), 1))\n",
    "\n",
    "        for item, value in enumerate(example[0]):\n",
    "            sentence_words_row = nltk.word_tokenize(value)\n",
    "            # separate each page with space\n",
    "            page_separate = re.sub(r'([A-Z])', r' \\1', example[1][item])\n",
    "            \n",
    "            page_words = nltk.word_tokenize(page_separate)\n",
    "            \n",
    "            # look for the same words from page appeared in sentence\n",
    "            same_words = [i for i in page_words if i in sentence_words_row]\n",
    "            \n",
    "            \n",
    "            X[item, :] = len(same_words)\n",
    "            \n",
    "            X = preprocessing.normalize(X, norm='l2')\n",
    "            return csr_matrix(X)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the FeatEngr class \n",
    "feat = FeatEngr()\n",
    "# feat.show_top10()\n",
    "# Train your Logistic Regression classifier and predict the accuracy\n",
    "feat.train_predict_model(random_state=1230)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [25 points] Problem 2: Motivation and Analysis \n",
    "***\n",
    "\n",
    "The job of the written portion of the homework is to convince the grader that:\n",
    "\n",
    "- Your new features work\n",
    "- You understand what the new features are doing\n",
    "- You had a clear methodology for incorporating the new features\n",
    "\n",
    "Make sure that you have examples and quantitative evidence that your features are working well. Be sure to explain how you used the data (e.g., did you have a validation set? did you do cross-validation?) and how you inspected the results. In addition, it is very important that you show some kind of an **error analysis** throughout your process.  That is, you should demonstrate that you've looked at misclassified examples and put thought into how you can craft new features to improve your model. \n",
    "\n",
    "A sure way of getting a low grade is simply listing what you tried and reporting the Kaggle score for each. You are expected to pay more attention to what is going on with the data and take a data-driven approach to feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2 answer:**                                                                       \n",
    "For the feature engineering homework, so far I've tried few things:                                                                                                          \n",
    "**Case1**, utilizing sentence column, using CountVectorizer method with bigram features, here called Feature1.                                                                                     \n",
    "**Case2**, utilizing sentence column, change CountVectorizer method to Tf-idf method to get rid of the unrelated words for normalization, here called Feature2.                                       \n",
    "**Case3**, using trope column to see which trope is considered as spoiler,here called Feature3.                                                                              \n",
    "**Case4**, combining page and sentence column, find out the words appeared in the sentence could be used to categorize spoiler or not, here called Feature4.                \n",
    "**Case5**, using featueunion to combine the feature 2 and feature 3 together.                                                                                                \n",
    "**Case6**, using featureunion to combine the feature 1, feature 2 and featuer 3.\n",
    "\n",
    "|$\\texttt{}$ | $\\texttt{Prediction Accuracy}$ | $\\texttt{Test Accuracy}$ |$\\texttt{Difference}$ | $\\texttt{Overfitting}$ \n",
    "|:----:|:----:|:----:|:----:|:----:|:----:|\n",
    "| **Case1**    |0.666 |0.659\t| 0.007|  No |\n",
    "| **Case2**    |0.683\t|0.661\t|0.022\t|  No |\n",
    "| **Case3**    |0.721\t|0.637\t|0.084\t|  No |\n",
    "| **Case4**    |0.529\t|0.521\t|0.008\t| No |\n",
    "| **Case5**    |0.729\t|0.716\t|0.019\t| No |\n",
    "| **Case6**    |0.740\t|0.701\t|0.039\t| Yes |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically what I did was that trying to predict the spoiler by using the sentence column itself at first, then looking around how the trope and page column related to the sentence column. What I found is that page column has the worst correlation to predict spoiler. In the case5 and case6, it seemed the model got improved quite a bit. However, in case6, there might be a bit overfitting by looking at the difference between the prediciton accuracy and test accuracy. The cross validation with five folds were preformed in this study.\n",
    "There are few other ways in the future to work around, for instance, gather additional training data and add more features (media genres) so we will have a bigger training pool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, there are some baseline features. For instance, the most straightforward features are the words present in the sentence. Also unigram and bigram features play a role in predicting the model, generally speaking, bigram performs better than uniform which makes sense because bigram uses consecutive pairs of tokens and results in a much bigger feture space. Below is the top seven unigram and bigram features based on the coefficient values.\n",
    "\n",
    "|$\\texttt{Unigram}$ | $\\texttt{Bigram}$ | \n",
    "|:----:|:----:|\n",
    "|killed\t|the end  |\n",
    "|freya \t|with the  |\n",
    "|dies \t|the season |\n",
    "|Harvey\t|turns out  |\n",
    "|turns\t |to kill  |\n",
    "|morgana  |\tin the  |\n",
    "\n",
    "                                                                                                                                                                                                                                                                 \n",
    "|$\\texttt{}$|$\\texttt{# of features}$ | $\\texttt{Accuracy}$ | \n",
    "|:----:|:----:|\n",
    "|Unigram\t|19080\t| 0.657 |\n",
    "|Bigram\t    |139022\t|  0.670 |\n",
    "\n",
    "Based on the number of features and accuracy shown above, bigram features is better at capturing contextual information with a larger feature space. \n",
    "\n",
    "Adding couple features is better than just using the baseline model, however, there is still errors. For example,              \n",
    "* *The first episode deals with  one of the season 1 bad guys getting killed by an  Eldritch Abomination  with telekinesis!* Our model incorrectly predict this as  a non-spoiler. It has the word, killed, the season, however, if you look at the corresponding trope column in the whole training set,  the spoiler predicts as not a spoiler. That is, our classifier is not able to leverage trope information contained in the sentence. In this case, we need more features, for instance, media genre to  improve the model.          \n",
    "\n",
    "* *After combing the feature together, Even after she returns in the middle of the charade.* Our classifier incorrectly predicted this sentence as  a spoiler. The reason is that the trope in which the sentence is categorized predicts more spoiler. In this case, filtering the original data is very necessary.                                                                                      \n",
    "\n",
    "* *Detective Paxson's partner, although not killed, was the one to take the fall for a political trap Michael set up for her.* our classifier predicted it as a spoiler because it got distracted by kill related words. However, this sentence is emphasizing the fact instead of the killing. \n",
    "\n",
    "Conclusion : Many sentences were mis-categorized because the lack of effective training data. Data cleaning should be performed in the very beginning. In this case adding features is definitely helpful in terms of enhancing the model accuracy, and what features should be added needs to be carefully filted to avoid overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "154px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
