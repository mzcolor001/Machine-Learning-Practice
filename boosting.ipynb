{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Homework \n",
    "***\n",
    "**Name**: $<$Juan Lin$>$ \n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday April 13th**. Submit only this Jupyter notebook to Moodle.  Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "In this homework you'll implement the AdaBoost classification framework to do handwritten digit recognition. Your implementation should be based on the description of AdaBoost given in the lecture slides.\n",
    "\n",
    "<br>\n",
    "\n",
    "![digits](mnist.png \"mnist data\")\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "Here are the rules: \n",
    "\n",
    "- Do **NOT** use sklearn's implementation of Adaboost.  You may however use sklearn's implementation of decisions trees. \n",
    "- Do **NOT** load or use any Python packages that are not available in Anaconda 3.6. \n",
    "- Some problems with code may be autograded.  If we provide a function or class API **do not** change it.\n",
    "- Do not change the location of the data or data directory.  Use only relative paths to access the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:16.624747Z",
     "start_time": "2018-04-02T09:58:15.916585Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.base import clone \n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Problem 1\n",
    "***\n",
    "\n",
    "Since we'll be working with binary classifiers, we'll look at the subset of the MNIST data pertaining to handwritten three's and eights. Note that we'll also be using a lower-res version of the MNIST data used in the KNN homework. The class below will load, parse, and store the subset of the. Load the data and then report: \n",
    "\n",
    "- The number of examples in the training set \n",
    "- The number of examples in the validation set \n",
    "- The number of pixels in each image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:18.840016Z",
     "start_time": "2018-04-02T09:58:18.805889Z"
    }
   },
   "outputs": [],
   "source": [
    "class ThreesAndEights:\n",
    "    \"\"\"\n",
    "    Class to store MNIST data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, location):\n",
    "\n",
    "        import pickle, gzip\n",
    "\n",
    "        # Load the dataset\n",
    "        f = gzip.open(location, 'rb')\n",
    "\n",
    "        # Split the data set \n",
    "        X_train, y_train, X_valid, y_valid = pickle.load(f)\n",
    "\n",
    "        # Extract only 3's and 8's for training set \n",
    "        self.X_train = X_train[np.logical_or( y_train==3, y_train == 8), :]\n",
    "        self.y_train = y_train[np.logical_or( y_train==3, y_train == 8)]\n",
    "        self.y_train = np.array([1 if y == 8 else -1 for y in self.y_train])\n",
    "        \n",
    "        # Shuffle the training data \n",
    "        shuff = np.arange(self.X_train.shape[0])\n",
    "        np.random.shuffle(shuff)\n",
    "        self.X_train = self.X_train[shuff,:]\n",
    "        self.y_train = self.y_train[shuff]\n",
    "\n",
    "        # Extract only 3's and 8's for validation set \n",
    "        self.X_valid = X_valid[np.logical_or( y_valid==3, y_valid == 8), :]\n",
    "        self.y_valid = y_valid[np.logical_or( y_valid==3, y_valid == 8)]\n",
    "        self.y_valid = np.array([1 if y == 8 else -1 for y in self.y_valid])\n",
    "        \n",
    "        print(\"the number of examples in the training set is:\", len(self.X_train))\n",
    "        print(\"the number of examples in the validation set is:\", len(self.X_valid))\n",
    "        print(\"the number of pixels in each image is:\", self.X_train.shape[1])\n",
    "       \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:19.740678Z",
     "start_time": "2018-04-02T09:58:19.609721Z"
    }
   },
   "outputs": [],
   "source": [
    "data = ThreesAndEights(\"./mnist21x21_3789.pklz\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Problem 2: Implementing AdaBoost  \n",
    "***\n",
    "\n",
    "We've given you a skeleton of the class `AdaBoost` below which will train a classifier based on boosted shallow decision trees as implemented by sklearn. Take a look at the class skeleton first so that you understand the underlying organization and data structures that we'll be using.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:23.101470Z",
     "start_time": "2018-04-02T09:58:23.016833Z"
    }
   },
   "outputs": [],
   "source": [
    "class AdaBoost:\n",
    "    def __init__(self, n_learners=20, base=DecisionTreeClassifier(max_depth=1), random_state=1234):\n",
    "        \"\"\"\n",
    "        Create a new adaboost classifier.\n",
    "        \n",
    "        Args:\n",
    "            N (int, optional): Number of weak learners in classifier.\n",
    "            base (BaseEstimator, optional): Your general weak learner \n",
    "            random_state (int, optional): set random generator.  needed for unit testing. \n",
    "\n",
    "        Attributes:\n",
    "            base (estimator): Your general weak learner \n",
    "            n_learners (int): Number of weak learners in classifier.\n",
    "            alpha (ndarray): Coefficients on weak learners. \n",
    "            learners (list): List of weak learner instances. \n",
    "        \"\"\"\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        \n",
    "        self.n_learners = n_learners \n",
    "        self.base = base\n",
    "        self.alpha = np.zeros(self.n_learners)\n",
    "        self.learners = []\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train AdaBoost classifier on data. Sets alphas and learners. \n",
    "        \n",
    "        Args:\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO \n",
    "\n",
    "        # Note: You can create and train a new instantiation \n",
    "        # of your sklearn decision tree as follows \n",
    "        # define w_i equals to 1/m\n",
    "        w = np.ones(len(y_train)) / len(y_train)\n",
    "        \n",
    "        # define err_k equals to 0\n",
    "        err_k = np.zeros(self.n_learners)\n",
    "        \n",
    "        for k in range(self.n_learners):\n",
    "            \n",
    "            h = clone(self.base)\n",
    "            self.learners.append(h)\n",
    "            \n",
    "            h.fit(X_train, y_train, sample_weight=w)\n",
    "            h_predict = h.predict(X_train)\n",
    "            \n",
    "        # compute weighted error err_k\n",
    "            for i in range(len(y_train)):\n",
    "                if h_predict[i] != y_train[i]:\n",
    "                    err_k[k] = err_k[k] + float((w[i]) / sum(w))\n",
    "        # compute alpha_k\n",
    "            self.alpha[k] = (1/2) * np.log(float(1 - err_k[k]) / err_k[k])\n",
    "        \n",
    "        # update w_i\n",
    "            for i in range(len(y_train)):\n",
    "                if h_predict[i] == y_train[i]:\n",
    "                    w[i] = w[i] * np.exp(-self.alpha[k])\n",
    "                else:\n",
    "                    w[i] = w[i] * np.exp(self.alpha[k])\n",
    "        \n",
    "        # make sure w = 1 using normalization\n",
    "            w = w / sum(w)\n",
    "        \n",
    "                 \n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Adaboost prediction for new data X.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            \n",
    "        Returns: \n",
    "            yhat (ndarray): [n_samples] ndarray of predicted labels {-1,1}\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO \n",
    "\n",
    "        prediction = np.zeros(X.shape[0])\n",
    "        for k in range(self.n_learners):\n",
    "            prediction += self.alpha[k] * self.learners[k].predict(X)\n",
    "        prediction = np.sign(prediction)\n",
    "            \n",
    "        return prediction\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes prediction accuracy of classifier.  \n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            y (ndarray): [n_samples] ndarray of true labels  \n",
    "            \n",
    "        Returns: \n",
    "            Prediction accuracy (between 0.0 and 1.0).\n",
    "        \"\"\"\n",
    "\n",
    "        \n",
    "        # TODO \n",
    "        score = 0\n",
    "        prediction_all = self.predict(X)\n",
    "        for item in range(len(y)):\n",
    "            if prediction_all[item] == y[item]:\n",
    "                score += 1\n",
    "        score = float(score/len(y))\n",
    "\n",
    "        return score\n",
    "        \n",
    "    \n",
    "    def staged_score(self, X, y):\n",
    "        \"\"\"\n",
    "        Computes the ensemble score after each iteration of boosting \n",
    "        for monitoring purposes, such as to determine the score on a \n",
    "        test set after each boost.\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            y (ndarray): [n_samples] ndarray of true labels  \n",
    "            \n",
    "        Returns: \n",
    "            scores (ndarary): [n_learners] ndarray of scores \n",
    "        \"\"\"\n",
    "\n",
    "        # TODO \n",
    "        st_score = np.zeros(self.n_learners)\n",
    "        prediction_k = np.zeros(len(y))\n",
    "        \n",
    "        for k in range(self.n_learners):\n",
    "            prediction_k += self.alpha[k] * self.learners[k].predict(X)\n",
    "            prediction_k_sign = np.sign(prediction_k)\n",
    "            for k1 in range(len(y)):\n",
    "                if prediction_k_sign[k1] == y[k1]:\n",
    "                    st_score[k] += 1\n",
    "            st_score[k] = float(st_score[k]/len(y))\n",
    "\n",
    "\n",
    "        return  st_score\n",
    "    \n",
    "    def staged_margin(self, x, y):\n",
    "        \"\"\"\n",
    "        Computes the staged margin after each iteration of boosting \n",
    "        for a single training example x and true label y\n",
    "        \n",
    "        Args:\n",
    "            x (ndarray): [n_features] ndarray of data \n",
    "            y (integer): an integer {-1,1} representing the true label of x \n",
    "            \n",
    "        Returns: \n",
    "            margins (ndarary): [n_learners] ndarray of margins \n",
    "        \"\"\"\n",
    "        \n",
    "        # TODO \n",
    "        \n",
    "        margins = []\n",
    "        margin_diff = 0\n",
    "        alpha_sum = 0\n",
    "\n",
    "        for k in range(0, len(self.learners)):\n",
    "            y_predict = self.learners[k].predict([x])\n",
    "            alpha_sum += self.alpha[k]\n",
    "            alpha_hat = self.alpha[k] / alpha_sum\n",
    "            if y_predict == y:\n",
    "                margin_diff += alpha_hat\n",
    "            else:\n",
    "                margin_diff -= alpha_hat\n",
    "            margins.append(margin_diff)\n",
    "       \n",
    "        return margins\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the model we attempt to learn in AdaBoost is given by \n",
    "\n",
    "$$\n",
    "H({\\bf x}) = \\textrm{sign}\\left[\\displaystyle\\sum_{k=1}^K\\alpha_k h_k({\\bf x}) \\right]\n",
    "$$\n",
    "\n",
    "where $h_k({\\bf x})$ is the $k^\\textrm{th}$ weak learner and $\\alpha_k$ is it's associated ensemble coefficient.  \n",
    "\n",
    "**Part A**: Implement the `fit` method to learn the sequence of weak learners $\\left\\{h_k({\\bf x})\\right\\}_{k=1}^K$ and corresponding coefficients $\\left\\{ \\alpha_k\\right\\}_{k=1}^K$. Note that you may use sklearn's implementation of DecisionTreeClassifier as your weak learner which allows you to pass as an optional parameter the weights associated with each training example.  An example of instantiating and training a single learner is given in the comments of the `fit` method.  \n",
    "\n",
    "When you think you're done, run the following unit tests which corresponds to the AdaBoost example given in the lecture slides. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:25.957948Z",
     "start_time": "2018-04-02T09:58:25.948471Z"
    }
   },
   "outputs": [],
   "source": [
    "%run -i tests.py \"part A\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: After your `fit` method is working properly, implement the `predict` method to make predictions for unseen examples stored in a data matrix ${\\bf X}$.  \n",
    "\n",
    "**Note**: Remember that AdaBoost assumes that your predictions are of the form $y \\in \\{-1, 1\\}$. \n",
    "\n",
    "When you think you're done, run the following unit tests which corresponds to the AdaBoost example given in the lecture slides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:29.697855Z",
     "start_time": "2018-04-02T09:58:29.689876Z"
    }
   },
   "outputs": [],
   "source": [
    "%run -i tests.py \"part B\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Next, implement the `score` method which takes in a matrix of examples ${\\bf X}$ and their associated true labels ${\\bf y}$, makes predictions, and returns the classification accuracy.   \n",
    "\n",
    "When you think you're done, run the following unit tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:32.327702Z",
     "start_time": "2018-04-02T09:58:32.320424Z"
    }
   },
   "outputs": [],
   "source": [
    "%run -i tests.py \"part C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Finally, implement the `staged_score` method to return an array of prediction accuracies after each iteration of the AdaBoost algorithm.  That is, the staged score array ${\\bf s}$ is defined such that ${\\bf s}_\\ell$ is the prediction accuracy using only the first $\\ell$ weak learners.  This function is primarily used as a diagnostic tool for analyzing the performance of your classifier during the training process.  \n",
    "\n",
    "**Note**: This method can be implemented in a very efficient or very **in**efficient matter.  Be sure to think about this a bit before diving in. \n",
    "\n",
    "\n",
    "When you think you're done, run the following unit tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:58:36.952096Z",
     "start_time": "2018-04-02T09:58:36.944919Z"
    }
   },
   "outputs": [],
   "source": [
    "%run -i tests.py \"part D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [10 points] Problem 3: AdaBoost for Handwritten Digit Recognition \n",
    "***\n",
    "\n",
    "Use your AdaBoost code with Sklearn's DecisionTreeClassifier as the base learner to distinguish $3$'s from $8$'s. \n",
    "Run $n=500$ boosting iterations with trees of depths 1, 2, and 3 (go deeper if you like) as the weak learner. For each weak learner, plot the training and validation error per boosting iteration (on the same set of axes). Compare and contrast the different weak learners. Which works the best? Do you see signs of overfitting? Do any of classifiers achieve nearly 100% accuracy on the training data? What happens to the accuracy on the validation data on further iterations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 3 Answer**                                                                                                               \n",
    "As shown in the picture below, tree depth = 1 works best, the overfitting starts much slower than the other two depths, and the error between training data and testing data is not that big compared to the other two depths. It is kinda a good bias and viariance trade-off. For all these three depths 1, 2, 3, I have seen signs of overfitting when they happen at different iteration round. For depth 1, it happens at around iteration round 200, for depth 2, it happens at around 30, and for depth 3, it happens at the very beginning. When depth is equal to 3, the training data can get a nearly 100% accuracy. Given further iterations, the accuracy on the test data increases slowly, tend to be stable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ThreesAndEights(\"./mnist21x21_3789.pklz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_1 = ['train accuracy with depth 1', 'train accuracy with depth 2', 'train accuracy with depth 3']\n",
    "labels_2 = ['validation accuracy with depth 1', 'validation accuracy with depth 2', 'validation accuracy with depth 3']\n",
    "\n",
    "for i in range(3):\n",
    "    digit_num = AdaBoost(n_learners=500, base=DecisionTreeClassifier(max_depth=i+1))\n",
    "    digit_num.fit(data.X_train, data.y_train)\n",
    "    \n",
    "    training_error = digit_num.staged_score(data.X_train, data.y_train)\n",
    "    valid_error = digit_num.staged_score(data.X_valid, data.y_valid)\n",
    "    x = np.linspace(0, 500, len(training_error))\n",
    "    plt.plot(x, training_error, color=\"blue\", label=labels_1[i])\n",
    "    plt.plot(x, valid_error, color=\"red\", label=labels_2[i])\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Problem 4: AdaBoost as a Margin-Maximizing Model \n",
    "***\n",
    "\n",
    "Despite the fact that we're making our model more complex with the addition of each weak learner, AdaBoost does not typically overfit the training data. The reason for this is that the model becomes more _confident_ with each boosting iteration. This _confidence_ can be interpreted mathematically as a margin. Recall that after $K$ iterations the algorithm terminates with the classifier \n",
    "\n",
    "$$\n",
    "H({\\bf x}) = \\textrm{sign}\\left[\\displaystyle\\sum_{k=1}^K\\alpha_k h_k({\\bf x}) \\right]\n",
    "$$\n",
    "\n",
    "Similarly, we can define the intermediate classifier $H_\\ell$ by \n",
    "\n",
    "$$\n",
    "H_\\ell({\\bf x}) = \\textrm{sign}\\left[\\displaystyle\\sum_{k=1}^\\ell\\alpha_k h_k({\\bf x}) \\right]\n",
    "$$\n",
    "\n",
    "where $\\ell \\leq K$. Note that in either case the model returns predictions of the form $y \\in \\{-1, 1\\}$ which does not give us any indication of the model's confidence in a prediction. Define the normalized coefficients $\\hat{\\alpha}_k$ as follows: \n",
    "\n",
    "$$\n",
    "\\hat{\\alpha}_k = \\dfrac{\\alpha_k}{\\sum_{t=1}^K \\alpha_k}\n",
    "$$\n",
    "\n",
    "Define the margin of a training example ${\\bf x}$ after $\\ell$ iterations as the sum of the normalized coefficients of weak learners that vote correctly minus the sum of the normalized coefficients of the weak learners that vote incorrectly: \n",
    "\n",
    "$$\n",
    "\\textrm{margin}_\\ell ({\\bf x}) = \\sum_{k=1:~h_k({\\bf x}) = y}^\\ell \\hat{\\alpha}_k - \\sum_{k=1:~h_k({\\bf x}) \\neq y}^\\ell \\hat{\\alpha}_k \n",
    "$$\n",
    "\n",
    "**Part A**: Briefly explain mathematically how $\\textrm{margin}_\\ell({\\bf x})$ can be interpreted as a margin.  **Hint**: You'll want to think back on what we meant by a _margin_ in our discussion of Support Vector Machines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Problem 4, Part A Answer** : In SVM, the margin was defined as \n",
    "<br>\n",
    "$$\n",
    "{\\bf w}^T{\\bf x^+} + b = +1\n",
    "$$\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "{\\bf w}^T{\\bf x^-} + b = -1\n",
    "$$\n",
    "<br>\n",
    "<br>\n",
    "$$\n",
    "{\\bf x^+} = {\\bf x^-} + {\\lambda}w\n",
    "$$\n",
    "<br>\n",
    "<br>$${\\bf M} = {\\bf x^-} + {\\lambda}w$$<br>\n",
    "<br>$$M = {\\bf |x^+ - x^-|} = |{\\lambda}w| = \\frac{1}{\\|{\\bf w}\\|}$$<br>\n",
    "So SVM is to maximize distance from the decision boundary to closet training datapoints and this closet distance is called the margin. Here $\\textrm{margin}_\\ell({\\bf x})$ is to maximize the distance of the sum of the weak learners which vote correctly and the sum of the weak learners which vote incorrectly. Therefore we could interpret it as a margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Complete the `staged_margin` method in the `AdaBoost` class above so that it computes the margin for a single training example ${\\bf x}$ after each boosting iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part C**: Find four **training** examples from the MNIST that meet the following criteria: \n",
    "\n",
    "- one $3$ that AdaBoost can classify easily  \n",
    "- one $8$ that AdaBoost can classify easily  \n",
    "- one $3$ that AdaBoost has difficulty with \n",
    "- one $8$ that AdaBoost has difficulty with \n",
    "\n",
    "Use the `view_digit` function given below to display the four examples that you found. \n",
    "\n",
    "**Advice**: Since AdaBoost will likely classify **all** training examples correctly given enough boosting iterations, you might try fitting an AdaBoost classifier with just a handful of boosting iterations and use it to identify examples of each desired type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4, Part C Answer**:                                                                                                                 \n",
    "one 3 that AdaBoost can classify easily is data.X_train[1]                                                                        \n",
    "one 8 that AdaBoost can classify easily is data.X_train[3]                                                                              \n",
    "one 3 that AdaBoost has difficulty with is data.X_train[105]                                                                          \n",
    "one 8 that AdaBoost has difficulty with is data.X_train[107]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-02T09:59:01.257421Z",
     "start_time": "2018-04-02T09:59:01.068539Z"
    }
   },
   "outputs": [],
   "source": [
    "def view_digit(example, label=None):\n",
    "    if label: \n",
    "        print(\"true label: {:d}\".format(label))\n",
    "    plt.imshow(example.reshape(21,21), cmap='gray');\n",
    "    \n",
    "#view_digit(data.X_train[3,:], data.y_train[3])\n",
    "view_digit(data.X_train[9,:], data.y_train[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_num = AdaBoost(n_learners=10, base=DecisionTreeClassifier(max_depth=2))\n",
    "digit_num.fit(data.X_train, data.y_train)\n",
    "diff = digit_num.predict(data.X_train) - data.y_train\n",
    "temp = []\n",
    "for i in range(len(diff)):\n",
    "    if diff[i] != 0:\n",
    "        temp.append(i)\n",
    "print(temp)\n",
    "view_digit(data.X_train[3,:], data.y_train[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D**: Using an AdaBoost classifier with at least $K=200$ depth-1 decision trees as the weak learners, plot the staged margin for each of the four examples that you found in **Part C** on the same set of axes. (Be sure to include a legend so we can tell which staged margin corresponds to which example).  Explain your results in terms of the margin of the classifier on each training examples.  More broadly, how the margin-maximizing property might allow AdaBoost to continue improving generalization even after the error on the training set reaches zero.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 4, Part D Answer:**\n",
    "Without any doubt,the staged margin for the items (3 easily classfied and 8 easily classified) have higher values than that of the items (3 hardly classfied and 8 hardly classify). Also the staged margin tends to change quite a bit in the first 50 iterations with the item of 8 easily classified compared to the item 3 easily classified and 3 hardly classified.                 \n",
    "When the training error is zero, then all examples are on the \"right side\" of the linear combination, here we view the hyperplan in a feature space where each base hypothesis is one feature/dimension. As soon as the training error is zero, the examples are on the right side and have positive margin. Therefore it is why AdaBoost will still learn even after the training error is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_num = AdaBoost(n_learners=200, base=DecisionTreeClassifier(max_depth=1))\n",
    "digit_num.fit(data.X_train, data.y_train)\n",
    "\n",
    "margin3_3 = digit_num.staged_margin(data.X_train[1,:], data.y_train[1])\n",
    "margin8_8 = digit_num.staged_margin(data.X_train[3,:], data.y_train[3])\n",
    "margin3_8 = digit_num.staged_margin(data.X_train[105,:], data.y_train[105])\n",
    "margin8_3 = digit_num.staged_margin(data.X_train[107,:], data.y_train[107])\n",
    "x = np.linspace(0, 200, len(margin3_3))\n",
    "plt.plot(x, margin3_3, color=\"blue\", label=\"3 easily classified\")\n",
    "plt.plot(x, margin8_8, color=\"red\", label=\"8 easily classified\")\n",
    "plt.plot(x, margin3_8, color=\"green\", label=\"3 hardly classified\")\n",
    "plt.plot(x, margin8_3, color=\"orange\", label=\"8 hardly classified\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"staged_margin\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), borderaxespad=0.)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
