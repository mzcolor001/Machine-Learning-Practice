{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Theory Homework \n",
    "***\n",
    "**Name**: $<$Juan Lin$>$ \n",
    "***\n",
    "\n",
    "This assignment is due on Moodle by **5pm on Friday March 9th**. Submit only this Jupyter notebook to Moodle. Do not compress it using tar, rar, zip, etc. Your solutions to analysis questions should be done in Markdown directly below the associated question.  Remember that you are encouraged to discuss the problems with your instructors and classmates, but **you must write all code and solutions on your own**.  For a refresher on the course **Collaboration Policy** click [here](https://github.com/chrisketelsen/CSCI5622-Machine-Learning/blob/master/resources/syllabus.md#collaboration-policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "***\n",
    "\n",
    "In this assignment you will explore the concepts of PAC learnability and VC dimension. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Problem 1: \n",
    "***\n",
    "\n",
    "Consider the class C of concepts defined by triangles with **distinct** vertices of the form $(i, j)$ where $i$ and $j$ are integers in the interval $[0,99]$. A concept c labels points on the interior and boundary of the associated triangle as positive and points exterior to the triangle as negative.\n",
    "\n",
    "**Note**: To make life easier, we'll allow degenerate triangles in $C$. That is, triangles where the vertices are collinear. The following image depicts an example of a nondegenerate and a degenerate triangle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"figs/triangles.png\" width=400 height=50>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A**: Suppose we have an algorithm that produces a consistent $h$ from the hypothesis class $H = C$. Give a bound on the number of training examples sufficient to assure that for any target concept $c$ in $C$, our algorithm will, with probability $1-\\delta$, output a hypothesis $h$ with generalization error at most $\\epsilon$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Part A Answer:** we will use the general bound for a consistent finite dimensional hypothesis class\n",
    "$$\n",
    "m \\geq \\frac{1}{\\epsilon}\\left(\\ln\\left| H \\right| + \\ln\\frac{1}{\\delta} \\right)\n",
    "$$\n",
    "\n",
    "Because we need pick up three vertices for a triangle, so $|H| = {{100*100}\\choose{3}} = 166616670000$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part B**: Based on your bound in **Part A**, determine the minimum number of training examples necessary such that for any target concept $c$ in $C$, our algorithm will, with probability $0.95$, output a hypothesis $h$ with generalization error at most $0.15$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Part B Answer**: For 99% accuracy we choose $\\epsilon = 0.15$ and for 95% confidence we choose $\\delta = 0.05$.  Then we have\n",
    "\n",
    "\n",
    "$$\n",
    "m \\geq \\frac{1}{0.15}\\left(28.14 + \\ln\\frac{1}{0.05} \\right) \\approx 193\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [15 points] Problem 2: \n",
    "***\n",
    "\n",
    "Consider feature vectors that live in two-dimensional space and the class of hypotheses defined by circles **centered at the origin**. There are two different kinds of hypotheses $h$ in this class. One type of hypthesis classifies points as positive if they lie on the boundary or **interior** of the circle, and negative otherwise. The other type of hypothesis classifies points as positive if they lie on the boundary or **exterior** of the circle, and negative otherwise. State and prove (rigorously) the VC dimension of this family of classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Problem 2 Answer:**\n",
    "$$\n",
    "h(x^2 + y^2) = x^2 + y^2 - r^2\n",
    "$$\n",
    "\n",
    "The lower bound gives $VCdim(H) \\leq 2.$\n",
    "\n",
    "prove that for all S s.t. |S|=3 there exists some labeling that S cannot be captured by H\n",
    "\n",
    "Let's say $A_1, A_2, A_3$ be arbitrary points and we have the distance from $A_1, A_2, A_3$ to the origin is $r_1, r_2, r_3$ and $r_1 < r_2 < r_3$. Let y= (+, -, +) and assume h = [0, r] works. \n",
    "because $y_1$, $y_3$ is positive, so $r_1 \\leq r$, $r_3 \\leq r$.\n",
    "\n",
    "so $r_1 < r_2 < r_3 < r$\n",
    "However because $y_2$ = - which means $r_2 \\geq  r$. There it is a contradiction.\n",
    "Therefore VCdim(H) = 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mycolors = {\"blue\":\"steelblue\", \"red\":\"#a76c6e\",  \"green\":\"#6a9373\", \"smoke\":\"#f2f2f2\"}\n",
    "circle1 = plt.Circle([0,0], 13**(1/2), color=mycolors[\"red\"], ls=\"--\", fill=False)\n",
    "circle2 = plt.Circle([0,0], 13**(1/2), color=mycolors[\"red\"], ls=\"--\", fill=False)\n",
    "circle3 = plt.Circle([0,0], 10**(1/2), color=mycolors[\"red\"], ls=\"--\", fill=False)\n",
    "circle4 = plt.Circle([0,0], 10**(1/2), color=mycolors[\"red\"], ls=\"--\", fill=False)\n",
    "fig, axes = plt.subplots(nrows=1, ncols=4, figsize=(20,4))\n",
    "axes[0].scatter([3,1], [2,3], color=[mycolors[\"red\"], mycolors[\"red\"]], s=150)\n",
    "axes[0].add_artist(circle1)\n",
    "axes[0].set_xlim([0,5]); axes[0].set_ylim([0,5]); axes[0].grid(alpha=0.25)\n",
    "axes[1].scatter([3,1], [2,3], color=[mycolors[\"blue\"], mycolors[\"blue\"]], s=150)\n",
    "axes[1].add_artist(circle2)\n",
    "axes[1].set_xlim([0,5]); axes[1].set_ylim([0,5]); axes[1].grid(alpha=0.25)\n",
    "axes[2].scatter([3,1], [2,3], color=[mycolors[\"red\"], mycolors[\"blue\"]], s=150)\n",
    "axes[2].add_artist(circle3)\n",
    "axes[2].set_xlim([0,5]); axes[2].set_ylim([0,5]); axes[2].grid(alpha=0.25)\n",
    "axes[3].scatter([3,1], [2,3], color=[mycolors[\"blue\"], mycolors[\"red\"]], s=150)\n",
    "axes[3].add_artist(circle4)\n",
    "axes[3].set_xlim([0,5]); axes[3].set_ylim([0,5]); axes[3].grid(alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [20 points] Problem 3: Empirical Verification of PAC Bounds for Axis-Aligned Rectangles \n",
    "***\n",
    "\n",
    "In the in-class notebook associated with PAC Learnability, we proved a PAC bound for the class of concepts $C$ comprised of axis-aligned rectangles living in $\\mathbb{R}^2$ of the form $(a \\leq x \\leq b) \\wedge (c \\leq y \\leq d)$ where $a, b, c, d$ are real numbers. Specifically, we proved that with probability $1-\\delta$, any consistent learner could learn a hypothesis $h$ in $H = C$ with generalization error less than $\\epsilon$ provided that the number of training examples satisfied \n",
    "\n",
    "$$\n",
    "m > \\frac{4}{\\epsilon}\\log\\frac{4}{\\delta}\n",
    "$$\n",
    "\n",
    "In this problem you will empirically verify this bound for the restricted concept class $C$ where the rectangles are defined by $(a \\leq x \\leq b) \\wedge (c \\leq y \\leq d)$ where $a, b, c, d$ are real numbers satisfying $0 \\leq a \\leq b \\leq 100$ and $0 \\leq c \\leq d \\leq 100$. \n",
    "\n",
    "**Part A**: The following is a general outline of how you should accomplish this, but it is up to you how you organize your code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Write some code that randomly generates a concept rectangle $c$. \n",
    "\n",
    "\n",
    "\n",
    "- Write some code that, given feature vectors of length-2, labels them according to some rectangle (that is, labels a point positive if the point is on the boundary or interior of the rectangle, and negative otherwise).  \n",
    "\n",
    "\n",
    "\n",
    "- Write some code that, given training examples of length-2, and labeled according to a concept $c$, returns a consistent hypothesis rectangle $h$. \n",
    "\n",
    "\n",
    "\n",
    "- Write some code that generates a training set of size $m$, labels them according to a random concept $c$, learns a consistent hypothesis $h$, and then approximates the generalization error by predicting on $1000$ new examples from the same distribution as the training data. \n",
    "\n",
    "\n",
    "- Write some code that computes approximate generalization errors for $100$ independent concepts $c$ and associated training sets of size $m$, and returns the worst-case generalization error at the confidence level $1-\\delta$.  One way to do this in the case that say $\\delta = 0.05$, is to report the $95^\\textrm{th}$ percentile of the $100$ samples of the generalization error. We can then say that, in our simulation, $100(1-\\delta)\\%$ of our observed generalization errors were less than our computed value. (**Bonus**: If your code is efficient, try increasing the number of runs in the simulation to $500$. This should give you a better approximation of the generalization error.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Part A - question 1, 2, 3 Answers **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import rand\n",
    "%matplotlib inline \n",
    "\n",
    "# generate rectangle\n",
    "def generate_rectangle_vertices():\n",
    "    x_0, y_0, x_1, y_1 = rand(4)\n",
    "    if x_0 > x_1:\n",
    "        x_0 , x_1 = x_1, x_0\n",
    "    if y_0 > y_1:\n",
    "        y_0, y_1 = y_1, y_0\n",
    "    return x_0, y_0, x_1, y_1\n",
    "\n",
    "def training_c(m):\n",
    "    # generate a concept rectangle c\n",
    "    x0_c, y0_c, x1_c, y1_c = generate_rectangle_vertices()\n",
    "#     print(x0_c,y0_c)\n",
    "#     print(x1_c, y1_c)\n",
    "    plt.plot([x0_c, x1_c], [y0_c, y0_c], color='red', ls='--')\n",
    "    plt.plot([x1_c, x1_c], [y0_c, y1_c], color='red', ls='--')\n",
    "    plt.plot([x1_c, x0_c], [y1_c, y1_c], color='red', ls='--')\n",
    "    plt.plot([x0_c, x0_c], [y1_c, y0_c], color='red', ls='--')\n",
    "\n",
    "\n",
    "    points_h = []\n",
    "    while len(points_h) == 0 :\n",
    "        x, y = rand(2, 100)\n",
    "#         plt.scatter(x, y, color='red')\n",
    "        for i, (a, b) in enumerate(zip(x,y)):\n",
    "            aa = (a,b)\n",
    "            if aa[0] >= x0_c:\n",
    "                if aa[0] <=x1_c:\n",
    "                    if aa[1] >=y0_c:\n",
    "                        if aa[1] <=y1_c:\n",
    "                            plt.scatter(aa[0], aa[1], color='green')\n",
    "                            points_h.append([aa[0], aa[1]])\n",
    "\n",
    "    x_range = []\n",
    "        \n",
    "    for i in points_h:\n",
    "        x_range.append(i[0])\n",
    "#         print(\"x_range\", x_range)\n",
    "    x_min = min(x_range)\n",
    "    x_max = max(x_range)\n",
    "\n",
    "    y_range = []\n",
    "    for j in points_h:\n",
    "        y_range.append(j[1])\n",
    "            \n",
    "#         print(\"y_range\", y_range)\n",
    "    y_min = min(y_range)\n",
    "    y_max = max(y_range)\n",
    "\n",
    "\n",
    "    plt.plot([x_min, x_max], [y_min, y_min], color='blue', ls='--')\n",
    "    plt.plot([x_max, x_max], [y_min, y_max], color='blue', ls='--')\n",
    "    plt.plot([x_max, x_min], [y_max, y_max], color='blue', ls='--')\n",
    "    plt.plot([x_min, x_min], [y_max, y_min], color='blue', ls='--')\n",
    "\n",
    "    x_test, y_test = rand(2, m)\n",
    "\n",
    "    \n",
    "#     mean = [0.5, 0.5]\n",
    "#     cov = [[0.625, 0], [0, 0.625]]\n",
    "#     x_test, y_test = np.random.multivariate_normal(mean, cov, m).T\n",
    "    \n",
    "    plt.scatter(x_test, y_test, color='black')\n",
    "    \n",
    "    #calculate the black points fall into the NOT overlapped region of C and H\n",
    "\n",
    "    points_test = []\n",
    "    for i, (a_test, b_test) in enumerate(zip(x_test,y_test)):\n",
    "        temp = (a_test,b_test)\n",
    "        if temp[0] >= x0_c:\n",
    "            if temp[0] <= x1_c:\n",
    "                if temp[1] >= y0_c:\n",
    "                    if temp[1] <= y1_c:\n",
    "\n",
    "                        points_test.append([temp[0], temp[1]])\n",
    "#     print(\"the number of points fall into C\", len(points_test))\n",
    "    \n",
    "    points_test_h = []\n",
    "    for i, (a_test, b_test) in enumerate(zip(x_test,y_test)):\n",
    "        temp1 = (a_test,b_test)\n",
    "        if temp1[0] >= x_min:\n",
    "            if temp1[0] <= x_max:\n",
    "                if temp1[1] >= y_min:\n",
    "                    if temp1[1] <= y_max:\n",
    "\n",
    "                        points_test_h.append([temp1[0], temp1[1]])\n",
    "\n",
    "    if len(points_test) == 0:\n",
    "        generation_error = 0\n",
    "    else:    \n",
    "        generation_error = (len(points_test) - len(points_test_h))/len(points_test)\n",
    "    plt.show()\n",
    "    \n",
    "    return generation_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part A - question 4 Answer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_error_onethou = training_c(1000)\n",
    "print(\"when the training set is 100, the generalization error when predict on 1000 new examples is:\", generation_error_onethou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Part A - question 5 Answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "gene_error = []\n",
    "\n",
    "for i in range(100):\n",
    "    gene_error.append(training_c(100))\n",
    "\n",
    "temp = np.mean(gene_error)\n",
    "gene_error = gene_error - temp\n",
    "gene_error = sorted(abs(gene_error), reverse=True)\n",
    "\n",
    "print(\"the worst-case generation error t the confidence level 0.95 is:\", gene_error[94])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Part B**: Use your code to estimate the generalization error with confidence parameter $\\delta=0.05$ for training sets of size $m$ where $m = 250, 500, 1000, 1250,$ and $1500$ and the data are comprised of points $(x,y)$ where the $x$- and $y$-values are sampled from the continuous uniform distribution $\\textrm{unif}(0,100)$. Make a **log-log** plot with $m$ on the horizontal axes and $\\epsilon$ on the vertical axis.  Additionally, overlay the theoretical PAC bound on your graph and discuss your results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Part B Answer:** Based on the graph below, we could see the the distribution error gets smaller as we increase the numbers of training data set, also the theorital generalization error and the predicted generization error have the same decreasing trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "gene_error_m = []\n",
    "\n",
    "for i in range(250 , 1500, 250):\n",
    "    gene_error = []\n",
    "    for j in range(100):\n",
    "        gene_error.append(training_c(i))\n",
    "    temp = np.mean(gene_error)\n",
    "    gene_error = gene_error - temp\n",
    "    gene_error = sorted(abs(gene_error), reverse=True)\n",
    "    \n",
    "   \n",
    "    precision = gene_error[95]\n",
    "    \n",
    "    gene_error_m.append(precision)\n",
    "    gene_error_m = sorted(gene_error_m, reverse=True)\n",
    "    \n",
    "print(gene_error_m)\n",
    "\n",
    "i = [i for i in range(250, 1500, 250)]\n",
    "epsilon = []\n",
    "for m in range(250, 1500, 250):\n",
    "    temp = ((4*(math.log(4/0.05)))/m)\n",
    "    epsilon.append(temp)\n",
    "print(epsilon)\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(i, [math.log(i) for i in gene_error_m],  color='red')\n",
    "plt.scatter(i, [math.log(i) for i in epsilon],  color='blue')\n",
    "            \n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Part C**: Repeat **Part B** where the data are comprised of points $(x,y)$ where the $x$- and $y$-values are sampled from the normal distribution with mean $\\mu = 50$ and standard deviation $\\sigma = 25$. Again, overlay the theoretical PAC bound on your graph and discuss your results. Do you expect to observe very different results than those observed in **Part B**?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Part C Answer:** We should expect the same trend with the theoritical generilization error and the predicted one, both decreased as we increase the number of the training dataset. However, compared the result with the Part B, we could see that predicted generilization error should be more consistent in terms of changing ratio because the test data is following the normal distribution N(50, 25) instead of N(0,1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from numpy.random import rand\n",
    "%matplotlib inline \n",
    "\n",
    "# generate rectangle\n",
    "def generate_rectangle_vertices():\n",
    "    x_0, y_0, x_1, y_1 = rand(4)\n",
    "    if x_0 > x_1:\n",
    "        x_0 , x_1 = x_1, x_0\n",
    "    if y_0 > y_1:\n",
    "        y_0, y_1 = y_1, y_0\n",
    "    return x_0, y_0, x_1, y_1\n",
    "\n",
    "def training_c(m):\n",
    "    # generate a concept rectangle c\n",
    "    x0_c, y0_c, x1_c, y1_c = generate_rectangle_vertices()\n",
    "#     print(x0_c,y0_c)\n",
    "#     print(x1_c, y1_c)\n",
    "    plt.plot([x0_c, x1_c], [y0_c, y0_c], color='red', ls='--')\n",
    "    plt.plot([x1_c, x1_c], [y0_c, y1_c], color='red', ls='--')\n",
    "    plt.plot([x1_c, x0_c], [y1_c, y1_c], color='red', ls='--')\n",
    "    plt.plot([x0_c, x0_c], [y1_c, y0_c], color='red', ls='--')\n",
    "\n",
    "\n",
    "    points_h = []\n",
    "    while len(points_h) == 0 :\n",
    "        x, y = rand(2, 100)\n",
    "#         plt.scatter(x, y, color='red')\n",
    "        for i, (a, b) in enumerate(zip(x,y)):\n",
    "            aa = (a,b)\n",
    "            if aa[0] >= x0_c:\n",
    "                if aa[0] <=x1_c:\n",
    "                    if aa[1] >=y0_c:\n",
    "                        if aa[1] <=y1_c:\n",
    "                            plt.scatter(aa[0], aa[1], color='green')\n",
    "                            points_h.append([aa[0], aa[1]])\n",
    "\n",
    "    x_range = []\n",
    "        \n",
    "    for i in points_h:\n",
    "        x_range.append(i[0])\n",
    "#         print(\"x_range\", x_range)\n",
    "    x_min = min(x_range)\n",
    "    x_max = max(x_range)\n",
    "\n",
    "    y_range = []\n",
    "    for j in points_h:\n",
    "        y_range.append(j[1])\n",
    "            \n",
    "#         print(\"y_range\", y_range)\n",
    "    y_min = min(y_range)\n",
    "    y_max = max(y_range)\n",
    "\n",
    "\n",
    "    plt.plot([x_min, x_max], [y_min, y_min], color='blue', ls='--')\n",
    "    plt.plot([x_max, x_max], [y_min, y_max], color='blue', ls='--')\n",
    "    plt.plot([x_max, x_min], [y_max, y_max], color='blue', ls='--')\n",
    "    plt.plot([x_min, x_min], [y_max, y_min], color='blue', ls='--')\n",
    "\n",
    "#     x_test, y_test = rand(2, m)\n",
    "\n",
    "    \n",
    "    mean = [0.5, 0.5]\n",
    "    cov = [[0.625, 0], [0, 0.625]]\n",
    "    x_test, y_test = np.random.multivariate_normal(mean, cov, m).T\n",
    "    \n",
    "    plt.scatter(x_test, y_test, color='black')\n",
    "    \n",
    "    #calculate the black points fall into the NOT overlapped region of C and H\n",
    "\n",
    "    points_test = []\n",
    "    for i, (a_test, b_test) in enumerate(zip(x_test,y_test)):\n",
    "        temp = (a_test,b_test)\n",
    "        if temp[0] >= x0_c:\n",
    "            if temp[0] <= x1_c:\n",
    "                if temp[1] >= y0_c:\n",
    "                    if temp[1] <= y1_c:\n",
    "\n",
    "                        points_test.append([temp[0], temp[1]])\n",
    "#     print(\"the number of points fall into C\", len(points_test))\n",
    "    \n",
    "    points_test_h = []\n",
    "    for i, (a_test, b_test) in enumerate(zip(x_test,y_test)):\n",
    "        temp1 = (a_test,b_test)\n",
    "        if temp1[0] >= x_min:\n",
    "            if temp1[0] <= x_max:\n",
    "                if temp1[1] >= y_min:\n",
    "                    if temp1[1] <= y_max:\n",
    "\n",
    "                        points_test_h.append([temp1[0], temp1[1]])\n",
    "\n",
    "    if len(points_test) == 0:\n",
    "        generation_error = 0\n",
    "    else:    \n",
    "        generation_error = (len(points_test) - len(points_test_h))/len(points_test)\n",
    "    plt.show()\n",
    "    \n",
    "    return generation_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "gene_error_m = []\n",
    "\n",
    "for i in range(250 , 1500, 250):\n",
    "    gene_error = []\n",
    "    for j in range(100):\n",
    "        gene_error.append(training_c(i))\n",
    "    temp = np.mean(gene_error)\n",
    "    gene_error = gene_error - temp\n",
    "    gene_error = sorted(abs(gene_error), reverse=True)\n",
    "    \n",
    "   \n",
    "    precision = gene_error[95]\n",
    "    \n",
    "    gene_error_m.append(precision)\n",
    "    gene_error_m = sorted(gene_error_m, reverse=True)\n",
    "\n",
    "    \n",
    "print(gene_error_m)\n",
    "\n",
    "i = [i for i in range(250, 1500, 250)]\n",
    "epsilon = []\n",
    "for m in range(250, 1500, 250):\n",
    "    temp = ((4*(math.log(4/0.05)))/m)\n",
    "    epsilon.append(temp)\n",
    "print(epsilon)\n",
    "\n",
    "\n",
    "\n",
    "plt.scatter(i, [math.log(i) for i in gene_error_m],  color='red')\n",
    "plt.scatter(i, [math.log(i) for i in epsilon],  color='blue')\n",
    "            \n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
